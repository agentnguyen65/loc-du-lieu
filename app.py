# T·∫†O FILE app.py (C·∫≠p nh·∫≠t v·ªõi File Uploader)
import streamlit as st
import re
import json
import io

# --- API Logic C·∫¨P NH·∫¨T (H·ªó tr·ª£ x·ª≠ l√Ω file) ---
def process_content(uploaded_file, text_input):
    """X·ª≠ l√Ω ƒë·∫ßu v√†o: ∆Øu ti√™n file, n·∫øu kh√¥ng c√≥ file th√¨ d√πng text_input."""
    content = ""
    if uploaded_file is not None:
        # Gi·∫£ ƒë·ªãnh file l√† vƒÉn b·∫£n (txt, csv)
        try:
            # ƒê·ªçc file nh·ªã ph√¢n v√† decode th√†nh string
            string_data = uploaded_file.getvalue().decode("utf-8")
            content = string_data
        except UnicodeDecodeError:
            st.error("L·ªói: Kh√¥ng th·ªÉ ƒë·ªçc file. Vui l√≤ng ƒë·∫£m b·∫£o ƒë√≥ l√† file vƒÉn b·∫£n (UTF-8).")
            return ""
    elif text_input:
        content = text_input
    
    return content

def generate_response(content_source, search_keyword):
    # API Logic core ƒë∆∞·ª£c gi·ªØ nguy√™n
    content = content_source
    keyword = search_keyword
    
    # ... (Ph·∫ßn c√≤n l·∫°i c·ªßa h√†m generate_response t·ª´ B∆∞·ªõc 2/3) ...
    # (ƒêo·∫°n m√£ n√†y kh√¥ng thay ƒë·ªïi)
    
    # Chu·∫©n b·ªã regex ƒë·ªÉ t√¨m t·ª´ kh√≥a ch√≠nh x√°c (M·ª©c ƒê·ªô Ch√≠nh X√°c: Tuy·ªát ƒë·ªëi)
    lines = content.split('\n')
    total_count = 0
    occurrences_list = []
    pattern = re.compile(r'\b' + re.escape(keyword) + r'\b', re.IGNORECASE)

    for i, line in enumerate(lines):
        matches = list(pattern.finditer(line))
        count_in_line = len(matches)
        total_count += count_in_line
        
        # ... (Ph·∫ßn t·∫°o occurrences_list v√† result) ...
        if count_in_line > 0:
            explanation = f"T·ª´ '{keyword}' xu·∫•t hi·ªán trong ƒëo·∫°n/d√≤ng th·ª© {i+1} c·ªßa n·ªôi dung."
            occurrences_list.append({
                "line_number": i + 1,
                "context_snippet": line.strip()[:100] + ('...' if len(line.strip()) > 100 else ''),
                "explanation_placeholder": explanation
            })
            
    result = {
        "summary_title": f"T·ªîNG K·∫æT L·ªåC D·ªÆ LI·ªÜU CHO T·ª™ KH√ìA '{keyword.upper()}'",
        "total_count": total_count,
        "occurrences_list": occurrences_list,
        "deploy_note": "C·∫ßn t√≠ch h·ª£p m√¥ h√¨nh AI l√µi ƒë·ªÉ t·∫°o 'Di·ªÖn Gi·∫£i R√µ R√†ng √ù Nghƒ©a' thay th·∫ø cho placeholder."
    }
    result["metadata_spg"] = {
        "Tieu_chi_Loc": "T·∫•t c·∫£ c√°c ch·ªØ s·ªë v√† k√≠ t·ª± ƒë·∫∑c bi·ªát.",
        "Muc_Do_Chinh_Xac": "Tuy·ªát ƒë·ªëi.",
        "Toc_Do_Phan_Hoi": "Nhanh ch√≥ng."
    }
    
    return result

# --- B·∫Øt ƒë·∫ßu X√¢y d·ª±ng UI Streamlit C·∫¨P NH·∫¨T ---
st.set_page_config(page_title="SPG Data Filter", layout="wide")
st.title("·ª®ng D·ª•ng L·ªçc D·ªØ Li·ªáu Ch√≠nh X√°c (SPG-Powered)")

# --- INPUT SECTION ---
with st.container():
    st.subheader("üì• B∆∞·ªõc 1 & 2: Ngu·ªìn ƒê·∫ßu V√†o & T·ª´ kh√≥a")
    
    col1, col2 = st.columns([3, 1])

    with col1:
        # THAY TH·∫æ TEXT AREA B·∫∞NG FILE UPLOADER & TEXT AREA
        uploaded_file = st.file_uploader(
            "1a. T·∫£i file vƒÉn b·∫£n (TXT, CSV)",
            type=['txt', 'csv'],
            help="∆Øu ti√™n d√πng file n√†y n·∫øu ƒë∆∞·ª£c t·∫£i l√™n."
        )
        
        text_input = st.text_area(
            "1b. Ho·∫∑c: D√°n ƒêo·∫°n VƒÉn B·∫£n L·ªõn",
            placeholder="D√°n n·ªôi dung l·ªõn c·∫ßn l·ªçc ·ªü ƒë√¢y...",
            height=150
        )
    
    with col2:
        search_keyword = st.text_input(
            "2. T·ª´ Kh√≥a Tra C·ª©u/L·ªçc",
            placeholder="V√≠ d·ª•: content"
        )
        
        # K√≠ch ho·∫°t API Logic khi nh·∫•n n√∫t
        if st.button("üöÄ T·∫°o K·∫øt Qu·∫£ L·ªçc Ch√≠nh X√°c"):
            processed_content = process_content(uploaded_file, text_input)
            
            if not processed_content or not search_keyword:
                st.error("Vui l√≤ng cung c·∫•p Ngu·ªìn n·ªôi dung (File ho·∫∑c Text) v√† T·ª´ kh√≥a.")
            else:
                # L∆∞u tr·ªØ k·∫øt qu·∫£ v√†o session state
                st.session_state['result'] = generate_response(processed_content, search_keyword)

# --- OUTPUT SECTION (Kh√¥ng thay ƒë·ªïi) ---
st.markdown("---")
st.subheader("‚úÖ B∆∞·ªõc 3: K·∫øt Qu·∫£ Ch√≠nh X√°c Tuy·ªát ƒê·ªëi")

if 'result' in st.session_state:
    result = st.session_state['result']

    if "error" in result:
        st.error(f"L·ªói: {result['error']}")
    else:
        st.success(result["summary_title"])
        st.metric(label="T·ªïng S·ªë L·∫ßn Xu·∫•t Hi·ªán", value=result["total_count"])
        
        st.markdown("**Chi Ti·∫øt V·ªã Tr√≠ Xu·∫•t Hi·ªán v√† Di·ªÖn Gi·∫£i:**")
        
        for item in result["occurrences_list"]:
            with st.expander(f"D√≤ng/ƒêo·∫°n {item['line_number']} (S·ªë l·∫ßn: 1)"):
                st.code(item['context_snippet'], language='text')
                st.info(f"Di·ªÖn Gi·∫£i (Placeholder): {item['explanation_placeholder']}")
        
        with st.expander("Metadata SPG (Tham s·ªë c·ªët l√µi)"):
            st.json(result["metadata_spg"])
else:
    st.info("Nh·∫•n n√∫t 'T·∫°o K·∫øt Qu·∫£ L·ªçc Ch√≠nh X√°c' ƒë·ªÉ xem k·∫øt qu·∫£.")
